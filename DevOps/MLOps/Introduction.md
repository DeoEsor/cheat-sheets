# SAI #08: Request-Response Model Deployment - The MLOps Way, Spark - Executor Memory Structure and more...

### Request-Response Model Deployment - The MLOps Way, Model Deployment - Scaling, Spark - Executor Memory Structure, Kafka - Data Replication (Basics).

In this episode we cover:

**MLOps.**

-   Request-Response Model Deployment - The MLOps Way.
    
-   Model Deployment - Scaling.
    

**Data Engineering.**

-   Spark - Executor Memory Structure.
    
-   Kafka - Data Replication (Basics).
    

**No Excuses Data Engineering Project Template.**

-   Making No Excuses Data Engineering Template Bulletproof.

---

### MLOps Fundamentals or What Every Machine Learning Engineer Should Know

---

#### _ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜-ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ - ğ—§ğ—µğ—² ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—ªğ—®ğ˜†._  
Â 

Last week we looked into a Model Deployment procedure as part of a Streaming Application.  
Â   
Today we look into deployment of the ML Model as a Request-Response API - The MLOps Way.  
Â   
ğ—Ÿğ—²ğ˜â€™ğ˜€ ğ˜‡ğ—¼ğ—¼ğ—º ğ—¶ğ—»:  
Â   
ğŸ­: Version Control: Machine Learning Training Pipeline is defined in code, once merged to the main branch it is built and triggered.  
ğŸ®: Feature Preprocessing: Features are retrieved from the Feature Store, validated and passed to the next stage. Any feature related metadata that is tightly coupled to the Model being trained is saved to the Experiment Tracking System.  
ğŸ¯: Model is trained and validated on Preprocessed Data, Model related metadata is saved to the Experiment Tracking System.  
ğŸ°.ğŸ­: If Model Validation passes all checks - Model Artifact is passed to a Model Registry.  
ğŸ°.ğŸ®: Model is packaged into a container ready to be exposed as REST or gRPC API. Model is Served.  
ğŸ±.ğŸ­: Experiment Tracking metadata is connected to Model Registry per Model Artifact. Responsible person chooses the best candidate and switches its state to Production.  
ğŸ±.ğŸ®: A web-hook is triggered by the action and a new version of containerised API is Deployed. We looked into release strategies in one of the previous posts.  
ğŸ²: A Request from a Product Application is performed against the API - Features for inference are retrieved from Real Time Feature Serving API and inference results are returned to the Application.  
ğŸ³: ML APIs are faced with a Load Balancer to enable horizontal scaling.  
ğŸ´: Multiple ML APIs will be exposed in this way to support different Product Applications. A good example is a Ranking Function.Â Â   
ğŸµ: Feature Store will be mounted on top of a Data Warehouse to retrieve Static Features or  
ğŸµ.ğŸ­: Some of the Features will be Dynamic and calculated in Real Time from Real Time Streaming Storage like Kafka.  
ğŸ­ğŸ¬: An orchestrator schedules Model Retraining.  
ğŸ­ğŸ­: ML Models that run in production are monitored. If Model quality degrades - retraining can be automatically triggered.  
Â   
[ğ—œğ— ğ—£ğ—¢ğ—¥ğ—§ğ—”ğ—¡ğ—§]: The Defined Flow assumes that your Pipelines are already Tested and ready to be released to Production. Weâ€™ll look into the pre-production flow in future episodes.  
Â   
ğ—£ğ—¿ğ—¼ğ˜€ ğ—®ğ—»ğ—± ğ—–ğ—¼ğ—»ğ˜€:  
Â   
âœ… Dynamic Features - available.  
âœ… Low Latency Inference.  
Â   
â—ï¸Inference results will be recalculated even if the input or result did not change.  
Â   
ğ—§ğ—µğ—¶ğ˜€ ğ—¶ğ˜€ ğ—§ğ—µğ—² ğ—ªğ—®ğ˜†.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd22191cc-f2d7-495a-8c16-2ce0dc2c5b60_3662x4665.png)



---

#### _ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜ - ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´._  

Â We already know the main types of Machine Learning Model Deployment that you will run into in real life situations: Batch, Embedded into a Stream Application and Request-Response.  
Â   
Today we look into how we scale these applications given an increase in Data Amount that needs to be processed.  
Â   
There are two different ways you can scale an Application:  
Â   
ğŸ‘‰ ğ—©ğ—²ğ—¿ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´ - you add resources to the container or server that is running the Application without increasing number of Applications.  
ğŸ‘‰ ğ—›ğ—¼ğ—¿ğ—¶ğ˜‡ğ—¼ğ—»ğ˜ğ—®ğ—¹ ğ—¦ğ—°ğ—®ğ—¹ğ—¶ğ—»ğ—´ - you increase the number of Applications that are performing required work.  
  
Letâ€™sÂ  look into what it means for each type of Model Deployment:  
  
ğ—•ğ—®ğ˜ğ—°ğ—µ.  
  
â¡ï¸ You Load Data from a Data Warehouse/Lake.  
â¡ï¸ Apply Inference.  
â¡ï¸ Save results to another Database.  
â¡ï¸ Product Applications can use the Inference Results from the Database.  
  
You can scale your Application in two ways.  
  
ğ™‘ğ™šğ™§ğ™©ğ™ğ™˜ğ™–ğ™¡.  
  
ğŸ‘‰ You add more resources to the container performing Inference.  
  
âœ… You are likely to have your models trained with libraries like scikit-learn, vertical is easy as you will not need to rewrite any code.  
  
ğ™ƒğ™¤ğ™§ğ™ğ™¯ğ™¤ğ™£ğ™©ğ™–ğ™¡.  
  
ğŸ‘‰ You perform Inference by leveraging a Distributed Compute Framework like Spark or Dask.  
  
ğ—¦ğ˜ğ—¿ğ—²ğ—®ğ—º.  
  
â¡ï¸ You Subscribe to a Kafka Topic.  
â¡ï¸ Apply Inference in Real Time on new data coming in.  
â¡ï¸ Save results to another Kafka Topic.  
â¡ï¸ Product Applications subscribe to the Resulting Topic.  
  
Stream Applications will almost always be scaled ğ—›ğ—¼ğ—¿ğ—¶ğ˜‡ğ—¼ğ—»ğ˜ğ—®ğ—¹ğ—¹ğ˜†.  
  
Scaling is influenced by two factors:  
  
ğŸ‘‰ Number of Partitions in the Input Stream - Number of Consumers canâ€™t be more than Number of Partitions. Any excess will Idle.  
ğŸ‘‰ Number of Consumers in the Consumer Group Reading from the Input Stream - you increase the number and Consumer Group takes care of rebalancing the Load.  
  
ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ - ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—².  
  
â¡ï¸ You expose ML Model as a REST or gRPC API.  
â¡ï¸ Product Applications are requesting for Inference results directly with the APIs in real time.  
  
A correct way to scale such APIs is ğ—›ğ—¼ğ—¿ğ—¶ğ˜‡ğ—¼ğ—»ğ˜ğ—®ğ—¹.  
  
ğŸ‘‰ We spin up new ML App containers.  
ğŸ‘‰ Wait for containers to become available - the endpoint should start returning Inference results Successfully.  
ğŸ‘‰ Register new containers with Load Balancer exposing them to a Product Application.  
Â   
ğ—§ğ—µğ—¶ğ˜€ ğ—¶ğ˜€ ğ—§ğ—µğ—² ğ—ªğ—®ğ˜†.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb9b8679-13f3-4981-807e-44c19deb7475_4051x5255.png)

---

### **Data Engineering Fundamentals + or What Every Data Engineer Should Know**

---

#### _ğ—¦ğ—½ğ—®ğ—¿ğ—¸ - ğ—˜ğ˜…ğ—²ğ—°ğ˜‚ğ˜ğ—¼ğ—¿ ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²._

Changing Spark Executor Memory configuration will most likely be the last step you would be taking to improve your Application Performance.  
  
Nevertheless, it is important to understand if you want to successfully troubleshoot Out Of Memory issues and understand why certain optimizations that you did in your application do not seem to work as expected.  
  
First of all, the entire memory container (JVM Heap Memory) is defined by a well known and widely used property ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—²ğ˜…ğ—²ğ—°ğ˜‚ğ˜ğ—¼ğ—¿.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†. It defines memory available for the remaining segments.Â   
  
There are four major segments that comprise Spark Executor memory, letâ€™s look closer:  
  
ğ—¥ğ—²ğ˜€ğ—²ğ—¿ğ˜ƒğ—²ğ—± ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜†:  
  
â¡ï¸ This is set to 300MB by default.Â   
â¡ï¸ You canâ€™t change it unless you recompile Spark.  
  
ğŸ‘‰ It is used to store Spark internal components  
  
â—ï¸ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—²ğ˜…ğ—²ğ—°ğ˜‚ğ˜ğ—¼ğ—¿.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜† canâ€™t be less than 1.5 times Reserved Memory.  
  
ğ—¨ğ˜€ğ—²ğ—¿ ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜†:  
  
â¡ï¸ This is set to (JVM Heap Memory - Reserved Memory) * (1 - ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ—³ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»).Â   
â¡ï¸ ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ—³ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» defaults to 0.75.  
  
ğŸ‘‰ It is used to store user defined structures like UDFs.  
  
ğ—¦ğ—½ğ—®ğ—¿ğ—¸ (ğ—¨ğ—»ğ—¶ğ—³ğ—¶ğ—²ğ—±) ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†:  
  
â¡ï¸ This is set to (JVM Heap Memory - Reserved Memory) * (1 - ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ—³ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»).Â   
  
This segment is further split into two parts.  
  
ğ™ğ™©ğ™¤ğ™§ğ™–ğ™œğ™š ğ™ˆğ™šğ™¢ğ™¤ğ™§ğ™®.  
  
â¡ï¸ This is set to (Spark (Unified) Memory) * ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ˜€ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—²ğ—™ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»  
â¡ï¸ ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ˜€ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—²ğ—™ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» defaults to 0.5  
  
ğŸ‘‰ It is used to store any Cached or Broadcasted Data if it is configured to be done In Memory.  
  
ğ™€ğ™­ğ™šğ™˜ğ™ªğ™©ğ™ğ™¤ğ™£ ğ™¢ğ™šğ™¢ğ™¤ğ™§ğ™®.  
  
â¡ï¸ This is set to (Spark (Unified) Memory) * (1 - ğ˜€ğ—½ğ—®ğ—¿ğ—¸.ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜†.ğ˜€ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—²ğ—™ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»)  
  
ğŸ‘‰ It is used to store any intermediate Data created by execution of Spark Jobs.  
  
â—ï¸Boundary between Storage and Execution memory is flexible.Â   
â—ï¸Execution Memory can always borrow memory from storage fraction.Â   
â—ï¸Storage Memory can only borrow from Execution if it is not occupied.  
â—ï¸If Execution Memory has borrowed from storage - Storage Memory can only reclaim it after it was released by Execution.  
â—ï¸Execution Memory can forcefully evict data from Storage Memory and claim it for itself.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F21e78360-b8f1-4ccd-9319-400c3fa05ec3_4180x3993.png)

---

#### _ğ—ğ—®ğ—³ğ—¸ğ—® - ğ——ğ—®ğ˜ğ—® ğ—¥ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—•ğ—®ğ˜€ğ—¶ğ—°ğ˜€)._  

Data Replication in Kafka is a mechanism that sits at the core of its reliability and durability guarantees - I am going to dedicate more than one post to explain the concepts behind it. As a Data Engineer you ğ— ğ—¨ğ—¦ğ—§ understand this deeply as it will impact the behavior of your Applications.  
  
Let's look into how replication works in Kafka.  
  
ğ—¦ğ—¼ğ—ºğ—² ğ—¿ğ—²ğ—³ğ—¿ğ—²ğ˜€ğ—µğ—²ğ—¿ğ˜€:  
  
â¡ï¸ Kafka Cluster is composed of Brokers.  
â¡ï¸ Data is stored in Topics that can be compared to tables in Databases.  
â¡ï¸ Topics are composed of Partitions.  
â¡ï¸ Clients writing Data to Kafka are called Producers.  
â¡ï¸ Clients reading Data from Kafka are called Consumers.  
â¡ï¸ Each Partition is and behaves as a Write Ahead Log - Producers always write to the end of a Partition.  
  
â¡ï¸ Kafka Cluster uses Zookeeper as an external central metadata store (being actively deprecated in favor of KRaft protocol for managing metadata internally).  
  
ğ——ğ—®ğ˜ğ—® ğ—¥ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»:  
  
â¡ï¸ Kafka is a Distributed System which means that Partitions of a single Topic will most likely and should be spread across different Brokers.  
â¡ï¸ Replication is a procedure when Partitions of Topics will be replicated a number of times.  
  
ğŸ‘‰ Replication factor is configured using ğ—¿ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—».ğ—³ğ—®ğ—°ğ˜ğ—¼ğ—¿ or ğ—±ğ—²ğ—³ğ—®ğ˜‚ğ—¹ğ˜.ğ—¿ğ—²ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—».ğ—³ğ—®ğ—°ğ˜ğ—¼ğ—¿ configuration - per topic and default for automatically created topics respectively.  
  
â¡ï¸ One of the partition replicas is defined as a Leader Partition.  
â¡ï¸ Data is always written to the Leader Partition and then is replicated to the followers.  
â¡ï¸ In most cases Data is also Read from Leader Partitions only. This means that replicas are only used for reliability reasons.  
â¡ï¸ Partition replicas can be in-sync or out of sync with the Leader Partition. In-sync means that Replica:  
  
ğŸ‘‰ Has an active session with Zookeeper.  
ğŸ‘‰ Fetched messages from the Leader Partition in the last n seconds (n is configurable).  
ğŸ‘‰ Had no lag behind the Leader Partition at least once in the last n seconds (n is configurable).  
  
â¡ï¸ Partition replicas are used in case of an emergency shutdown of a broker or a planned upgrade when a downtime is needed.  
â¡ï¸ If a Broker containing Leader Replicas goes offline - a new in-sync Replica will be elected as a new Leader retaining the normal operation of the Topic.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc780fc7-cda1-4e99-9a89-7d01cfa1a577_3978x4989.png)

---

### Making No Excuses Data Engineering Project Template Bulletproof

---

Today we look into how we could make the entire ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º of ğ—§ğ—µğ—² ğ—§ğ—²ğ—ºğ—½ğ—¹ğ—®ğ˜ğ—² more robust.  
Â   
Here are some points ğ—¬ğ—¼ğ˜‚ should take into consideration and ğ—ªğ—² will build into the implementation of ğ—§ğ—µğ—² ğ—§ğ—²ğ—ºğ—½ğ—¹ğ—®ğ˜ğ—² when bringing it to life:  
Â   
â¡ï¸Â  Make ğ—–ğ—¼ğ—¹ğ—¹ğ—²ğ—°ğ˜ğ—¼ğ—¿ (2.) and ğ—˜ğ—»ğ—¿ğ—¶ğ—°ğ—µğ—ºğ—²ğ—»ğ˜ ğ—”ğ—£ğ—œ (4.) ğ—›ğ—¶ğ—´ğ—µğ—¹ğ˜† ğ—”ğ˜ƒğ—®ğ—¶ğ—¹ğ—®ğ—¯ğ—¹ğ—² - front the application with a ğ—Ÿğ—¼ğ—®ğ—± ğ—•ğ—®ğ—¹ğ—®ğ—»ğ—°ğ—²ğ—¿ and deploy multiple replicas of The Collector.  
Â   
ğŸ‘‰ Bonus points for introducing autoscaling.  
Â   
â¡ï¸Â  Ensure that each Application that reads from Kafka (3., 5., 6.) can do so using multiple instances of application deployed simultaneously.  
Â   
ğŸ‘‰ We already covered ğ—–ğ—¼ğ—»ğ˜€ğ˜‚ğ—ºğ—²ğ—¿ ğ—šğ—¿ğ—¼ğ˜‚ğ—½ğ˜€ in one of the episodes. We will use them.  
Â   
â¡ï¸Â  Implement centralized logging for your Real Time Applications (2.- 6.).Â   
Â   
ğŸ‘‰ Use FluentD sidecar containers that would pass application logs to a separate index in ElasticSearch (T5.).  
ğŸ‘‰ MountÂ  Kibana on top of Elasticsearch for easy Log Access.  
Â   
â¡ï¸Â  Implement centralized Application Metric collection for any Python application (2.- 6.).  
Â   
ğŸ‘‰ Use Prometheus Server (T7.) to collect metrics from the applications.  
ğŸ‘‰ Mount Grafana on top of Prometheus for convenient Metrics exploration.  
ğŸ‘‰ We will also use these metrics to implement autoscaling.  
Â   
â¡ï¸Â  Implement ğ—”ğ—¹ğ—²ğ—¿ğ˜ğ—¶ğ—»ğ—´ on data in Dead Letter Topics.  
Â   
ğŸ‘‰ Use either a separate Stream Processing Application or pipe data to Elasticsearch for Real Time Access and calculate alerting metrics on an interval.  
Â Â   
All of the above are a minimum must haves for a robust and stable system. In some of the next episodes we will be doing deeper dives into separate components of The Template top-down so ğ—ğ—²ğ—²ğ—½ ğ˜ğ˜‚ğ—»ğ—²ğ—± ğ—¶ğ—»!


![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeb98463-b459-4246-a783-f68e259bc726_3985x5777.png)